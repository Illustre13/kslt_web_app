{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bde245",
   "metadata": {},
   "source": [
    "# 1. Improt  and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9134b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099aee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import  mediapipe  as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d003d",
   "metadata": {},
   "source": [
    "# 2. Keypoint using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a8f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic Model\n",
    "mp_drawing  = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0d7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) #converting color\n",
    "    image.flags.writeable = False # Image is no longer writeable\n",
    "    results = model.process(image) #make prediction\n",
    "    image.flags.writeable = True # Image is now writeable\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) #converting color\n",
    "    return image,results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f22715",
   "metadata": {},
   "source": [
    "# This is optional now,The one being used is the draw_styled_landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26a2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, result):\n",
    "    mp_drawing.draw_landmarks(image,result.pose_landmarks,mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(250,250,250), thickness = 2, circle_radius = 2),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness = 1, circle_radius = 1)) # Draw pose conections\n",
    "    mp_drawing.draw_landmarks(image,result.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS) # Draw left Hand conections\n",
    "    mp_drawing.draw_landmarks(image,result.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS) # Draw Right Hand conection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f80f5f",
   "metadata": {},
   "source": [
    "# Optional {For testing the camera and detection mechanisim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37393d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap  = cv2.VideoCapture(1)\n",
    "# # Accessing mediapipe model\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "        \n",
    "#         # Read feed\n",
    "#         ret,frame = cap.read()\n",
    "\n",
    "#         # Make detection\n",
    "#         image, results = mediapipe_detection(frame,holistic)\n",
    "        \n",
    "        \n",
    "#         # Draw  landmarks\n",
    "#         draw_styled_landmarks(image,results)\n",
    "        \n",
    "\n",
    "#         cv2.imshow('RSL Feed', image)\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c576058",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42860eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# With out Face points  multiple left and hand cordinates to increase accuracy\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # magnifiying the hands value for better accuracy\n",
    "    return np.concatenate([pose, lh, rh, lh, rh, lh, rh, lh, rh, lh, rh, lh, rh, lh, rh, lh, rh, lh, rh, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dba315",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de929a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: ..\\signlang-kinyarwanda/Data\n"
     ]
    }
   ],
   "source": [
    "# Output will be  arrray containing numerical values for the extracted keypoints\n",
    "# Path to outr external data\n",
    "DATA_PATH = os.path.join('..', 'signlang-kinyarwanda/Data')  # Adjust path as needed\n",
    "\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.isdir(DATA_PATH):\n",
    "    print(\"Folder exists:\", DATA_PATH)\n",
    "else:\n",
    "    print(\"Folder NOT found:\", DATA_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c8c345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 actions in the dataset:\n",
      "['---' 'akarere' 'akazi' 'amakuru' 'bayi' 'bibi' 'igihugu' 'neza'\n",
      " 'nyarugenge' 'oya' 'papa' 'umujyi wa kigali' 'umurenge' 'urakoze' 'yego']\n"
     ]
    }
   ],
   "source": [
    "# Get all action folders from the external dataset\n",
    "action_folders = [f for f in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, f))]\n",
    "actions = np.array(sorted(action_folders))  # Sort for consistency\n",
    "\n",
    "print(f\"Found {len(actions)} actions in the dataset:\")\n",
    "print(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9700a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Info:\n",
      "- Number of sequences per action: 40\n",
      "- Frames per sequence: 30\n",
      " Now we have a total of 600 videos from all actions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Auto-detect number of sequences and frames\n",
    "sample_action = actions[0]\n",
    "sample_path = os.path.join(DATA_PATH, sample_action)\n",
    "sequences_in_action = [d for d in os.listdir(sample_path) if os.path.isdir(os.path.join(sample_path, d))]\n",
    "no_sequences = len(sequences_in_action)\n",
    "\n",
    "# Check frames in first sequence\n",
    "first_sequence_path = os.path.join(sample_path, sequences_in_action[0])\n",
    "npy_files = [f for f in os.listdir(first_sequence_path) if f.endswith('.npy')]\n",
    "sequence_length = len(npy_files)\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"- Number of sequences per action: {no_sequences}\")\n",
    "print(f\"- Frames per sequence: {sequence_length}\")\n",
    "\n",
    "print(\" Now we have a total of {} videos from all actions.\".format(len(actions)*no_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff93897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA STRUCTURE INSPECTION\n",
      "============================================================\n",
      "\n",
      "Action: ---\n",
      "  Total sequences: 40\n",
      "  First sequence: 1\n",
      "    Number of .npy files: 30\n",
      "    Sample file shape: (1518,)\n",
      "    Feature dimensions: 1518\n",
      "\n",
      "Action: akarere\n",
      "  Total sequences: 40\n",
      "  First sequence: 1\n",
      "    Number of .npy files: 30\n",
      "    Sample file shape: (1518,)\n",
      "    Feature dimensions: 1518\n",
      "\n",
      "Action: akazi\n",
      "  Total sequences: 40\n",
      "  First sequence: 1\n",
      "    Number of .npy files: 30\n",
      "    Sample file shape: (1518,)\n",
      "    Feature dimensions: 1518\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data structure\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA STRUCTURE INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for action in actions[:3]:  # Check first 3 actions\n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    sequences = sorted([d for d in os.listdir(action_path) if os.path.isdir(os.path.join(action_path, d))])\n",
    "    \n",
    "    print(f\"\\nAction: {action}\")\n",
    "    print(f\"  Total sequences: {len(sequences)}\")\n",
    "    \n",
    "    # Check first sequence\n",
    "    if sequences:\n",
    "        first_seq_path = os.path.join(action_path, sequences[0])\n",
    "        npy_files = sorted([f for f in os.listdir(first_seq_path) if f.endswith('.npy')])\n",
    "        \n",
    "        print(f\"  First sequence: {sequences[0]}\")\n",
    "        print(f\"    Number of .npy files: {len(npy_files)}\")\n",
    "        \n",
    "        # Load one file to check shape\n",
    "        if npy_files:\n",
    "            sample_file = os.path.join(first_seq_path, npy_files[0])\n",
    "            sample_data = np.load(sample_file)\n",
    "            print(f\"    Sample file shape: {sample_data.shape}\")\n",
    "            print(f\"    Feature dimensions: {sample_data.shape[0] if len(sample_data.shape) == 1 else sample_data.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402568dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d183cc2a",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1dff2a",
   "metadata": {},
   "source": [
    "# SKIPPED\n",
    "**Note:** Data collection is skipped because we're using pre-existing `.npy` files from the external dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af6592",
   "metadata": {},
   "source": [
    "# 6. Preprocess External Data and Create Labels and Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5202b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014f087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "  0: ---\n",
      "  1: akarere\n",
      "  2: akazi\n",
      "  3: amakuru\n",
      "  4: bayi\n",
      "  5: bibi\n",
      "  6: igihugu\n",
      "  7: neza\n",
      "  8: nyarugenge\n",
      "  9: oya\n",
      "  10: papa\n",
      "  11: umujyi wa kigali\n",
      "  12: umurenge\n",
      "  13: urakoze\n",
      "  14: yego\n",
      "\n",
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create label map\n",
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "print(\"Label Mapping:\")\n",
    "for label, num in label_map.items():\n",
    "    print(f\"  {num}: {label}\")\n",
    "\n",
    "# Load sequences and labels\n",
    "sequences, labels = [], []\n",
    "missing_data = []\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "for action in actions:\n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    \n",
    "    # Get all sequence folders\n",
    "    sequence_folders = sorted([d for d in os.listdir(action_path) \n",
    "                               if os.path.isdir(os.path.join(action_path, d))])\n",
    "    \n",
    "    for sequence_folder in sequence_folders:\n",
    "        sequence_path = os.path.join(action_path, sequence_folder)\n",
    "        window = []\n",
    "        \n",
    "        try:\n",
    "            # Get all .npy files in sequence\n",
    "            npy_files = sorted([f for f in os.listdir(sequence_path) if f.endswith('.npy')])\n",
    "            \n",
    "            if len(npy_files) != sequence_length:\n",
    "                missing_data.append(f\"{action}/{sequence_folder} - Expected {sequence_length}, found {len(npy_files)}\")\n",
    "                continue\n",
    "            \n",
    "            # Load each frame\n",
    "            for npy_file in npy_files:\n",
    "                npy_path = os.path.join(sequence_path, npy_file)\n",
    "                res = np.load(npy_path)\n",
    "                window.append(res)\n",
    "            \n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "            \n",
    "        except Exception as e:\n",
    "            missing_data.append(f\"{action}/{sequence_folder} - Error: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nLoaded {len(sequences)} sequences successfully\")\n",
    "\n",
    "if missing_data:\n",
    "    print(f\"\\nWarning: {len(missing_data)} sequences had issues:\")\n",
    "    for item in missing_data[:10]:  # Show first 10\n",
    "        print(f\"  - {item}\")\n",
    "    if len(missing_data) > 10:\n",
    "        print(f\"  ... and {len(missing_data) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80634ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Shapes:\n",
      "Sequences array shape: (351, 30, 1518)\n",
      "Labels array shape: (351,)\n",
      "\n",
      "Feature dimensions per frame: 1518\n",
      "Expected: 1392 (Phase 3 with magnified hands)\n",
      "\n",
      "Label Distribution:\n",
      "  ---: 40 sequences\n",
      "  akarere: 40 sequences\n",
      "  akazi: 40 sequences\n",
      "  amakuru: 40 sequences\n",
      "  bayi: 40 sequences\n",
      "  bibi: 40 sequences\n",
      "  igihugu: 40 sequences\n",
      "  neza: 40 sequences\n",
      "  yego: 31 sequences\n"
     ]
    }
   ],
   "source": [
    "# Verify loaded data\n",
    "print(\"\\nData Shapes:\")\n",
    "print(f\"Sequences array shape: {np.array(sequences).shape}\")\n",
    "print(f\"Labels array shape: {np.array(labels).shape}\")\n",
    "\n",
    "# Check feature dimensions\n",
    "if len(sequences) > 0:\n",
    "    print(f\"\\nFeature dimensions per frame: {len(sequences[0][0])}\")\n",
    "    print(f\"Expected: 1392 (Phase 3 with magnified hands)\")\n",
    "    \n",
    "    # Distribution of labels\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    for label_idx, count in zip(unique_labels, counts):\n",
    "        action_name = actions[label_idx]\n",
    "        print(f\"  {action_name}: {count} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5723d",
   "metadata": {},
   "source": [
    "# 7. Convert to arrays and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shapes:\n",
      "X (features): (351, 30, 1518)\n",
      "y (labels): (351, 15)\n",
      "\n",
      "Actual feature dimensions: 1518\n",
      "\n",
      "Train-Test Split:\n",
      "Training samples: 245\n",
      "Testing samples: 106\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"X (features): {X.shape}\")  # Should be (num_sequences, 30, feature_dim)\n",
    "print(f\"y (labels): {y.shape}\")    # Should be (num_sequences, num_actions)\n",
    "\n",
    "# Get actual feature dimensions\n",
    "actual_feature_dim = X.shape[2]\n",
    "print(f\"\\nActual feature dimensions: {actual_feature_dim}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "print(f\"\\nTrain-Test Split:\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d146c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8b9bf22",
   "metadata": {},
   "source": [
    "# 8. Build Model with Correct Input Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaae935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STUDENT\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">405,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">495</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │       \u001b[38;5;34m405,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m495\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">560,207</span> (2.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m560,207\u001b[0m (2.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">560,207</span> (2.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m560,207\u001b[0m (2.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "# Setup callbacks\n",
    "log_dir = os.path.join('Logs_External_Data')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Optimizer\n",
    "opt = Adam(learning_rate=0.00001)\n",
    "\n",
    "# Model architecture - UPDATE INPUT SHAPE\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu',\n",
    "               kernel_regularizer=regularizers.l2(0.01), \n",
    "               input_shape=(sequence_length, actual_feature_dim)))  # Use actual dimensions\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu', \n",
    "               kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu', \n",
    "               kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(len(actions), activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48aee8",
   "metadata": {},
   "source": [
    "# 9. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541da13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - categorical_accuracy: 0.9668 - loss: 7.2723 - val_categorical_accuracy: 0.8491 - val_loss: 7.8648\n",
      "Epoch 2/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - categorical_accuracy: 0.9725 - loss: 7.2653 - val_categorical_accuracy: 0.8491 - val_loss: 7.8637\n",
      "Epoch 3/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - categorical_accuracy: 0.9803 - loss: 7.2394 - val_categorical_accuracy: 0.8491 - val_loss: 7.9038\n",
      "Epoch 4/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - categorical_accuracy: 0.9733 - loss: 7.2451 - val_categorical_accuracy: 0.8491 - val_loss: 7.8765\n",
      "Epoch 5/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - categorical_accuracy: 0.9763 - loss: 7.2511 - val_categorical_accuracy: 0.8491 - val_loss: 7.8943\n",
      "Epoch 6/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - categorical_accuracy: 0.9734 - loss: 7.2389 - val_categorical_accuracy: 0.8491 - val_loss: 7.9136\n",
      "Epoch 7/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - categorical_accuracy: 0.9826 - loss: 7.1746 - val_categorical_accuracy: 0.8491 - val_loss: 7.9306\n",
      "Epoch 8/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - categorical_accuracy: 0.9790 - loss: 7.1694 - val_categorical_accuracy: 0.8396 - val_loss: 7.9291\n",
      "Epoch 9/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - categorical_accuracy: 0.9825 - loss: 7.1738 - val_categorical_accuracy: 0.8491 - val_loss: 7.9370\n",
      "Epoch 10/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - categorical_accuracy: 0.9889 - loss: 7.1683 - val_categorical_accuracy: 0.8491 - val_loss: 7.9377\n",
      "Epoch 11/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 200ms/step - categorical_accuracy: 0.9953 - loss: 7.1275 - val_categorical_accuracy: 0.8396 - val_loss: 7.9524\n",
      "Epoch 12/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 189ms/step - categorical_accuracy: 0.9986 - loss: 7.1000 - val_categorical_accuracy: 0.8396 - val_loss: 7.9467\n",
      "Epoch 13/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - categorical_accuracy: 0.9901 - loss: 7.1257 - val_categorical_accuracy: 0.8396 - val_loss: 7.9497\n",
      "Epoch 14/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - categorical_accuracy: 0.9936 - loss: 7.1081 - val_categorical_accuracy: 0.8491 - val_loss: 7.9492\n",
      "Epoch 15/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - categorical_accuracy: 0.9980 - loss: 7.0665 - val_categorical_accuracy: 0.8491 - val_loss: 7.9625\n",
      "Epoch 16/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - categorical_accuracy: 0.9973 - loss: 7.0566 - val_categorical_accuracy: 0.8491 - val_loss: 7.9485\n",
      "Epoch 17/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - categorical_accuracy: 0.9965 - loss: 7.0470 - val_categorical_accuracy: 0.8491 - val_loss: 7.9584\n",
      "Epoch 18/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - categorical_accuracy: 0.9980 - loss: 7.0394 - val_categorical_accuracy: 0.8491 - val_loss: 7.9069\n",
      "Epoch 19/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - categorical_accuracy: 0.9973 - loss: 7.0302 - val_categorical_accuracy: 0.8491 - val_loss: 7.8815\n",
      "Epoch 20/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - categorical_accuracy: 0.9683 - loss: 7.1855 - val_categorical_accuracy: 0.7830 - val_loss: 8.5405\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=200,\n",
    "    validation_data=(X_test, y_test),  # Use test set for validation\n",
    "    callbacks=[tb_callback, early_stop],\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8dc91",
   "metadata": {},
   "source": [
    "## Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['categorical_accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85288fd2",
   "metadata": {},
   "source": [
    "Mak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab4d52",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766e182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDL_HW1P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
